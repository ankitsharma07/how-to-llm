{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "ea035aa6e1914d3a889238e4944748d1",
            "04c174ca4a2042febab7b2722f39094c",
            "5dced4329d4e45a08d9df29bddf727c0",
            "5eb76005d7004cd68505cb733bd03a3e",
            "5899b5b82b3c4a57878fff9e250d9cc2",
            "234237048fc8440e87b89d884ca4c2ba",
            "70f5d0e37dad4ac295058095bcf4b37e",
            "0298afd49407463a8343af570b5559dc",
            "04b0c5c6be02481e80639aac8f90c2e5",
            "f8a556eeb6504acc9185a386215d7fc7",
            "82d12e97901549b2b666370c802d9280",
            "3ddac7a351f5429191f2da6bdeffe1b0",
            "4c0064a521d44c738b9f8a02f0676fca",
            "5e2073cc48ec470b811a19cf524dc1d3"
          ]
        },
        "id": "nClu0QSc4HXs",
        "outputId": "685a3a7e-7c49-4f0a-ca3e-29c55059b491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea035aa6e1914d3a889238e4944748d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJPQksG64jwO",
        "outputId": "44323c3c-4078-4955-a8a9-db04c55a4737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.24 in /usr/local/lib/python3.10/dist-packages (1.24.0)\n",
            "Requirement already satisfied: sentencepiece==0.1.98 in /usr/local/lib/python3.10/dist-packages (0.1.98)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q einops\n",
        "!pip install numpy==1.24\n",
        "!pip install sentencepiece==0.1.98\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrCh-hDv440A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "a3e34dea245e4536a94db1897d1f1e4e",
            "a83fca16ac4444f1a09e2cb136af1b8e",
            "faad8277fc0c4e54abde25a8fb6bce27",
            "e674401403004746807af85626ec30d8",
            "7c4665174325491789014d052bc9df0e",
            "e08177adea504f5e84dd11dd6dd85c3a",
            "0edc9ae8a9f24098a9d5e2f0aa85ea9b",
            "f06b0d93264c426b879d5eb6c2cdb802",
            "be5254474c0d4d919cf655ef0c904b60",
            "2b4d77408cdd48369f26f412403313ca",
            "fa0c611f002a4b928845b5cff16db15b",
            "98bb412076f944758f556be28c502679",
            "fdff279416654bc8a7af2c074aae3c2f",
            "d249d82f04b14a609bc03034e832156c",
            "1a05991216104fd1afb579397fd4600b",
            "01c6d59642e54b91997487ddd344a9de",
            "131c2d2e8eda419a86c2476a401e12bf",
            "52692a2d741e4ecbb98328a304344914",
            "417a4eefe03044d99f8413612d8601c0",
            "a24f91722da543ebafee5bfe44355ab2",
            "422d48bb5c1848fd86a1791ec42f7b62",
            "642f0463720441dca0b4f40ea574fde1",
            "36d632f5562044fd88e871cc7002c3b8",
            "ca2260dc74b34286937c130287bbebe7",
            "a549c51862e94b8ea44bce6a1fd5b8c7",
            "23f789e3e1d2465fbacb48ed234d806e",
            "570c05454066427499b33b8305d289ee",
            "6264895659d74c08bf0a11fe4ef2451b",
            "d522e19358464f0e985741ea429e183e",
            "a1d2cad6d24248d2ab0838d3db1da2ab",
            "a981ddf7e86040bd949d046a91523163",
            "b7680a634cb441d8b617baedec1fffc3",
            "2ef7a8fab7974e76b20b764e58e37294"
          ]
        },
        "id": "3_4vN-wh5uNz",
        "outputId": "7d29c4c8-2f71-4237-f455-a3e8da02bc41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3e34dea245e4536a94db1897d1f1e4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98bb412076f944758f556be28c502679",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36d632f5562044fd88e871cc7002c3b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "model_name,\n",
        "trust_remote_code=True,\n",
        "torch_dtype=torch.bfloat16,\n",
        "device_map= 'auto',\n",
        "offload_folder='offload'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuAAprOj7uBr",
        "outputId": "d604c453-ea8e-4dfd-dfbd-71ecd15ef88d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 17492, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 17492 (delta 62), reused 99 (delta 51), pack-reused 17370\u001b[K\n",
            "Receiving objects: 100% (17492/17492), 20.74 MiB | 25.56 MiB/s, done.\n",
            "Resolving deltas: 100% (12124/12124), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxnPbfsA708I",
        "outputId": "c8ee3c02-20a0-4746-abbd-d0550493f4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6MBBIT88BVX"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('./models/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abzQhgHP-HXh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def download_file_from_huggingface(model_name, filename, save_path):\n",
        "  url = f\"https://huggingface.co/{model_name}/resolve/main/{filename}\"\n",
        "  r = requests.get(url)\n",
        "  if r.status_code != 200:\n",
        "\n",
        "    print(f\"Failed to download {filename}.HTTP Status Code: {r.status_code}\")\n",
        "    return False\n",
        "  with open(os.path.join(save_path, filename), \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czt5AYKs-Hai",
        "outputId": "1989adc0-50df-4ea1-f88d-04c782a34827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully downloaded tokenizer_config.json\n",
            "Successfully downloaded tokenizer.model\n",
            "Successfully downloaded tokenizer.json\n",
            "Successfully downloaded special_tokens_map.json\n",
            "Successfully downloaded added_tokens.json\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "  files_to_download = [\n",
        "      \"tokenizer_config.json\",\n",
        "      \"tokenizer.model\",\n",
        "      \"tokenizer.json\",\n",
        "      \"special_tokens_map.json\",\n",
        "      \"added_tokens.json\"\n",
        "      ]\n",
        "  # Create the directory if it doesn't exist\n",
        "  save_path=\"./models\"\n",
        "  if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "  # Download the files\n",
        "  for filename in files_to_download:\n",
        "      success = download_file_from_huggingface(model_name, filename, save_path)\n",
        "      if success:\n",
        "        print(f\"Successfully downloaded {filename}\")\n",
        "      else:\n",
        "        print(f\"Failed to download {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4ucXwc0-Hdc",
        "outputId": "f4f467da-8e6e-409f-f656-a041a2f83c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Waiting for headers] [1 InRelease 0 B/3,626 \u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Waiting for headers] [Connecting to ppa.laun\u001b[0m\r                                                                                                    \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [2 InRelease 14.2 kB/110 kB 13%] [Connecting \u001b[0m\r                                                                                                    \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [665 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,676 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,391 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,060 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,722 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [23.8 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [44.4 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [66.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,326 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,677 kB]\n",
            "Fetched 9,910 kB in 2s (5,417 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "83 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libopenblas-dev is already the newest version (0.3.20+ds-1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
            "Suggested packages:\n",
            "  libeigen3-doc libmpfrc++-dev\n",
            "The following NEW packages will be installed:\n",
            "  libeigen3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 83 not upgraded.\n",
            "Need to get 1,056 kB of archives.\n",
            "After this operation, 9,081 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeigen3-dev all 3.4.0-2ubuntu2 [1,056 kB]\n",
            "Fetched 1,056 kB in 0s (2,605 kB/s)\n",
            "Selecting previously unselected package libeigen3-dev.\n",
            "(Reading database ... 121671 files and directories currently installed.)\n",
            "Preparing to unpack .../libeigen3-dev_3.4.0-2ubuntu2_all.deb ...\n",
            "Unpacking libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Setting up libeigen3-dev (3.4.0-2ubuntu2) ...\n"
          ]
        }
      ],
      "source": [
        "!apt update -y\n",
        "!apt install build-essential git cmake libopenblas-dev libeigen3-dev -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKt2qsWbBuq6",
        "outputId": "7b078230-951b-43bc-d5cd-97e4d931ae4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:   -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/imatrix/imatrix.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o imatrix -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas  -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/passkey/passkey.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o -o passkey -L/usr/lib/x86_64-linux-gnu/openblas-pthread/ -lopenblas \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENBLAS -I/usr/include/x86_64-linux-gnu/openblas-pthread/  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native  -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
          ]
        }
      ],
      "source": [
        "!make LLAMA_OPENBLAS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GkH-U7y-CJ6n"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eFrSrOq9CJ9i",
        "outputId": "56e87d52-8a7f-43d1-e6e9-d1be6e887078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model file models/model.safetensors\n",
            "params = Params(n_vocab=32003, n_embd=2048, n_layer=22, n_ctx=2048, n_ff=5632, n_head=32, n_head_kv=4, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('models'))\n",
            "Found vocab files: {'tokenizer.model': PosixPath('models/tokenizer.model'), 'vocab.json': None, 'tokenizer.json': PosixPath('models/tokenizer.json')}\n",
            "Loading vocab file 'models/tokenizer.model', type 'spm'\n",
            "Vocab info: <SentencePieceVocab with 32000 base tokens and 3 added tokens>\n",
            "Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0, 'pad': 32000}, add special tokens unset>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "lm_head.weight                                   -> output.weight                            | BF16   | [32003, 2048]\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32003, 2048]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [2048]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [2048, 5632]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [5632, 2048]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [5632, 2048]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [256, 2048]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [2048, 2048]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [2048, 2048]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [256, 2048]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [2048]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [2048, 5632]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [5632, 2048]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [5632, 2048]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [2048]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [256, 2048]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [2048, 2048]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [2048, 2048]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [256, 2048]\n",
            "model.norm.weight                                -> output_norm.weight                       | BF16   | [2048]\n",
            "Writing models/ggml-model-f16.gguf, format 1\n",
            "Ignoring added_tokens.json since model matches vocab size without it.\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting special token type pad to 32000\n",
            "[  1/201] Writing tensor output.weight                          | size  32003 x   2048  | type F16  | T+   1\n",
            "[  2/201] Writing tensor token_embd.weight                      | size  32003 x   2048  | type F16  | T+   2\n",
            "[  3/201] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   2\n",
            "[  4/201] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   2\n",
            "[  5/201] Writing tensor blk.0.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   2\n",
            "[  6/201] Writing tensor blk.0.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   2\n",
            "[  7/201] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   2\n",
            "[  8/201] Writing tensor blk.0.attn_k.weight                    | size    256 x   2048  | type F16  | T+   2\n",
            "[  9/201] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   2\n",
            "[ 10/201] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   2\n",
            "[ 11/201] Writing tensor blk.0.attn_v.weight                    | size    256 x   2048  | type F16  | T+   3\n",
            "[ 12/201] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   3\n",
            "[ 13/201] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+   3\n",
            "[ 14/201] Writing tensor blk.1.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+   3\n",
            "[ 15/201] Writing tensor blk.1.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+   4\n",
            "[ 16/201] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   4\n",
            "[ 17/201] Writing tensor blk.1.attn_k.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "[ 18/201] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   4\n",
            "[ 19/201] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   4\n",
            "[ 20/201] Writing tensor blk.1.attn_v.weight                    | size    256 x   2048  | type F16  | T+   4\n",
            "[ 21/201] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   4\n",
            "[ 22/201] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   4\n",
            "[ 23/201] Writing tensor blk.10.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   4\n",
            "[ 24/201] Writing tensor blk.10.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "[ 25/201] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "[ 26/201] Writing tensor blk.10.attn_k.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "[ 27/201] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 28/201] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 29/201] Writing tensor blk.10.attn_v.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "[ 30/201] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   5\n",
            "[ 31/201] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   5\n",
            "[ 32/201] Writing tensor blk.11.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   5\n",
            "[ 33/201] Writing tensor blk.11.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   5\n",
            "[ 34/201] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   5\n",
            "[ 35/201] Writing tensor blk.11.attn_k.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "[ 36/201] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 37/201] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   5\n",
            "[ 38/201] Writing tensor blk.11.attn_v.weight                   | size    256 x   2048  | type F16  | T+   5\n",
            "[ 39/201] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   5\n",
            "[ 40/201] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   5\n",
            "[ 41/201] Writing tensor blk.12.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   6\n",
            "[ 42/201] Writing tensor blk.12.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   6\n",
            "[ 43/201] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   6\n",
            "[ 44/201] Writing tensor blk.12.attn_k.weight                   | size    256 x   2048  | type F16  | T+   6\n",
            "[ 45/201] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   6\n",
            "[ 46/201] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   6\n",
            "[ 47/201] Writing tensor blk.12.attn_v.weight                   | size    256 x   2048  | type F16  | T+   8\n",
            "[ 48/201] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   8\n",
            "[ 49/201] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+   8\n",
            "[ 50/201] Writing tensor blk.13.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+   9\n",
            "[ 51/201] Writing tensor blk.13.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+   9\n",
            "[ 52/201] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+   9\n",
            "[ 53/201] Writing tensor blk.13.attn_k.weight                   | size    256 x   2048  | type F16  | T+   9\n",
            "[ 54/201] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+   9\n",
            "[ 55/201] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   9\n",
            "[ 56/201] Writing tensor blk.13.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "[ 57/201] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "[ 58/201] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  10\n",
            "[ 59/201] Writing tensor blk.14.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  10\n",
            "[ 60/201] Writing tensor blk.14.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  10\n",
            "[ 61/201] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "[ 62/201] Writing tensor blk.14.attn_k.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "[ 63/201] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "[ 64/201] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "[ 65/201] Writing tensor blk.14.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "[ 66/201] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "[ 67/201] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  10\n",
            "[ 68/201] Writing tensor blk.15.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  10\n",
            "[ 69/201] Writing tensor blk.15.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  10\n",
            "[ 70/201] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+  10\n",
            "[ 71/201] Writing tensor blk.15.attn_k.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "[ 72/201] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+  10\n",
            "[ 73/201] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  10\n",
            "[ 74/201] Writing tensor blk.15.attn_v.weight                   | size    256 x   2048  | type F16  | T+  10\n",
            "[ 75/201] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+  10\n",
            "[ 76/201] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  11\n",
            "[ 77/201] Writing tensor blk.16.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  11\n",
            "[ 78/201] Writing tensor blk.16.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  11\n",
            "[ 79/201] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+  11\n",
            "[ 80/201] Writing tensor blk.16.attn_k.weight                   | size    256 x   2048  | type F16  | T+  12\n",
            "[ 81/201] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+  12\n",
            "[ 82/201] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  14\n",
            "[ 83/201] Writing tensor blk.16.attn_v.weight                   | size    256 x   2048  | type F16  | T+  14\n",
            "[ 84/201] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+  14\n",
            "[ 85/201] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  14\n",
            "[ 86/201] Writing tensor blk.17.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  14\n",
            "[ 87/201] Writing tensor blk.17.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  14\n",
            "[ 88/201] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+  15\n",
            "[ 89/201] Writing tensor blk.17.attn_k.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "[ 90/201] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+  15\n",
            "[ 91/201] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  15\n",
            "[ 92/201] Writing tensor blk.17.attn_v.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "[ 93/201] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+  15\n",
            "[ 94/201] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  15\n",
            "[ 95/201] Writing tensor blk.18.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  15\n",
            "[ 96/201] Writing tensor blk.18.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  15\n",
            "[ 97/201] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+  15\n",
            "[ 98/201] Writing tensor blk.18.attn_k.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "[ 99/201] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+  15\n",
            "[100/201] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  15\n",
            "[101/201] Writing tensor blk.18.attn_v.weight                   | size    256 x   2048  | type F16  | T+  15\n",
            "[102/201] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+  15\n",
            "[103/201] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  15\n",
            "[104/201] Writing tensor blk.19.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  16\n",
            "[105/201] Writing tensor blk.19.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  16\n",
            "[106/201] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+  16\n",
            "[107/201] Writing tensor blk.19.attn_k.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "[108/201] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+  16\n",
            "[109/201] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  16\n",
            "[110/201] Writing tensor blk.19.attn_v.weight                   | size    256 x   2048  | type F16  | T+  16\n",
            "[111/201] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+  16\n",
            "[112/201] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  16\n",
            "[113/201] Writing tensor blk.2.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  16\n",
            "[114/201] Writing tensor blk.2.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  19\n",
            "[115/201] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+  19\n",
            "[116/201] Writing tensor blk.2.attn_k.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "[117/201] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+  19\n",
            "[118/201] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  19\n",
            "[119/201] Writing tensor blk.2.attn_v.weight                    | size    256 x   2048  | type F16  | T+  19\n",
            "[120/201] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+  19\n",
            "[121/201] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  19\n",
            "[122/201] Writing tensor blk.20.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  20\n",
            "[123/201] Writing tensor blk.20.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  20\n",
            "[124/201] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "[125/201] Writing tensor blk.20.attn_k.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "[126/201] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+  20\n",
            "[127/201] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  20\n",
            "[128/201] Writing tensor blk.20.attn_v.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "[129/201] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+  20\n",
            "[130/201] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5632  | type F16  | T+  20\n",
            "[131/201] Writing tensor blk.21.ffn_gate.weight                 | size   5632 x   2048  | type F16  | T+  20\n",
            "[132/201] Writing tensor blk.21.ffn_up.weight                   | size   5632 x   2048  | type F16  | T+  20\n",
            "[133/201] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "[134/201] Writing tensor blk.21.attn_k.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "[135/201] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+  20\n",
            "[136/201] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+  20\n",
            "[137/201] Writing tensor blk.21.attn_v.weight                   | size    256 x   2048  | type F16  | T+  20\n",
            "[138/201] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+  20\n",
            "[139/201] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  21\n",
            "[140/201] Writing tensor blk.3.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  21\n",
            "[141/201] Writing tensor blk.3.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  21\n",
            "[142/201] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+  21\n",
            "[143/201] Writing tensor blk.3.attn_k.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "[144/201] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+  21\n",
            "[145/201] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  21\n",
            "[146/201] Writing tensor blk.3.attn_v.weight                    | size    256 x   2048  | type F16  | T+  21\n",
            "[147/201] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+  21\n",
            "[148/201] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  22\n",
            "[149/201] Writing tensor blk.4.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  22\n",
            "[150/201] Writing tensor blk.4.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  22\n",
            "[151/201] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+  22\n",
            "[152/201] Writing tensor blk.4.attn_k.weight                    | size    256 x   2048  | type F16  | T+  22\n",
            "[153/201] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+  22\n",
            "[154/201] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  24\n",
            "[155/201] Writing tensor blk.4.attn_v.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "[156/201] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+  24\n",
            "[157/201] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  24\n",
            "[158/201] Writing tensor blk.5.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  24\n",
            "[159/201] Writing tensor blk.5.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  24\n",
            "[160/201] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+  24\n",
            "[161/201] Writing tensor blk.5.attn_k.weight                    | size    256 x   2048  | type F16  | T+  24\n",
            "[162/201] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+  24\n",
            "[163/201] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  25\n",
            "[164/201] Writing tensor blk.5.attn_v.weight                    | size    256 x   2048  | type F16  | T+  25\n",
            "[165/201] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+  25\n",
            "[166/201] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  25\n",
            "[167/201] Writing tensor blk.6.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  25\n",
            "[168/201] Writing tensor blk.6.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  25\n",
            "[169/201] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+  25\n",
            "[170/201] Writing tensor blk.6.attn_k.weight                    | size    256 x   2048  | type F16  | T+  25\n",
            "[171/201] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+  25\n",
            "[172/201] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  25\n",
            "[173/201] Writing tensor blk.6.attn_v.weight                    | size    256 x   2048  | type F16  | T+  25\n",
            "[174/201] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+  25\n",
            "[175/201] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  25\n",
            "[176/201] Writing tensor blk.7.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  25\n",
            "[177/201] Writing tensor blk.7.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  25\n",
            "[178/201] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+  25\n",
            "[179/201] Writing tensor blk.7.attn_k.weight                    | size    256 x   2048  | type F16  | T+  25\n",
            "[180/201] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+  25\n",
            "[181/201] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  25\n",
            "[182/201] Writing tensor blk.7.attn_v.weight                    | size    256 x   2048  | type F16  | T+  25\n",
            "[183/201] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+  25\n",
            "[184/201] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  26\n",
            "[185/201] Writing tensor blk.8.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  26\n",
            "[186/201] Writing tensor blk.8.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  26\n",
            "[187/201] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+  27\n",
            "[188/201] Writing tensor blk.8.attn_k.weight                    | size    256 x   2048  | type F16  | T+  27\n",
            "[189/201] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+  27\n",
            "[190/201] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  27\n",
            "[191/201] Writing tensor blk.8.attn_v.weight                    | size    256 x   2048  | type F16  | T+  27\n",
            "[192/201] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+  27\n",
            "[193/201] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5632  | type F16  | T+  27\n",
            "[194/201] Writing tensor blk.9.ffn_gate.weight                  | size   5632 x   2048  | type F16  | T+  28\n",
            "[195/201] Writing tensor blk.9.ffn_up.weight                    | size   5632 x   2048  | type F16  | T+  28\n",
            "[196/201] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+  28\n",
            "[197/201] Writing tensor blk.9.attn_k.weight                    | size    256 x   2048  | type F16  | T+  28\n",
            "[198/201] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+  28\n",
            "[199/201] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+  28\n",
            "[200/201] Writing tensor blk.9.attn_v.weight                    | size    256 x   2048  | type F16  | T+  28\n",
            "[201/201] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+  28\n",
            "Wrote models/ggml-model-f16.gguf\n"
          ]
        }
      ],
      "source": [
        "!python convert.py models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SpWh8pydBuvB",
        "outputId": "14ec36b0-0fa9-467a-b36b-8a5224c40e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing models/TinyLlama-1.1B-Chat-v0.3.Q4_K.gguf with Q4_K quantization.\n"
          ]
        }
      ],
      "source": [
        "# Split the string by the '/' character\n",
        "parts = model_name.split('/')\n",
        "\n",
        "# Get the second part (index 1 because Python is 0-based)\n",
        "model_name_pure = parts [1]\n",
        "\n",
        "quant_type = \"Q4_K\"\n",
        "quantized_model = f'models/{model_name_pure}.{quant_type}.gguf'\n",
        "\n",
        "print(f'Preparing {quantized_model} with {quant_type} quantization.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ia8k1S1CDeFh",
        "outputId": "358a99f7-80a3-4ccd-f086-dee777973917"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletedProcess(args=['./quantize', 'models/ggml-model-f16.gguf', 'models/TinyLlama-1.1B-Chat-v0.3.Q4_K.gguf', 'Q4_K'], returncode=0)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import subprocess\n",
        "# Build the command as a list of arguments\n",
        "command = [\"./quantize\", \"models/ggml-model-f16.gguf\", quantized_model, quant_type]\n",
        "# Execute the command\n",
        "subprocess.run(command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dm7iSo3OGAVy"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "quant_name = model_name.split('/') [-1]+\"-GGUF\" # Splits by '/' and takes the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1NkRK8aSLXBK",
        "outputId": "f875570d-a73d-4ca6-d26e-8c874b6bf1e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TinyLlama-1.1B-Chat-v0.3-GGUF'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "quant_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SH3YeISLSYP"
      },
      "outputs": [],
      "source": [
        "\"Ayaansh/TinyLlama-1.1B-Chat-v0.3-GGUF\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i5RByZxUHUsx"
      },
      "outputs": [],
      "source": [
        "# Specify the repository where you want to upload the files\n",
        "repo_id = \"Ayaansh/\" + quant_name\n",
        "base_path = \"./models\"\n",
        "# Array of local file paths you want to upload\n",
        "local_file_paths = [\n",
        "        base_path + \"/special_tokens_map.json\",\n",
        "        base_path + \"/tokenizer_config.json\",\n",
        "        base_path + \"/tokenizer.json\",\n",
        "        base_path + \"/tokenizer.model\",\n",
        "        base_path + \"/ggml-vocab-llama.gguf\",\n",
        "        base_path + \"/tokenizer.json\",\n",
        "        base_path + \"/\" + f'{model_name_pure}.{quant_type}.gguf',\n",
        "        ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCUkxVOhHyso"
      },
      "outputs": [],
      "source": [
        "# Loop through each file and upload it\n",
        "for local_file_path in local_file_paths:\n",
        "  # Extract the file name from the local file path\n",
        "  file_name = local_file_path.split(\"/\")[-1]\n",
        "  print(file_name)\n",
        "  # Specify the path where you want the file to be uploaded in the repository\n",
        "  path_in_repo = file_name # Using file_name directly, adjust as needed\n",
        "  #Upload the file\n",
        "  api.upload_file(\n",
        "                  path_or_fileobj=local_file_path,\n",
        "                  path_in_repo=path_in_repo,\n",
        "                  repo_id=repo_id,\n",
        "                  repo_type=\"model\",\n",
        "  )\n",
        "  print (f\"Uploaded {file_name} to {repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "8067fa0d19a841bf9e2d8010e5b41538",
            "381d5df4f32242fbba2bdb114626e79a",
            "7ad5e754de4947158b7a082deaf58c77",
            "0b8c87b555ba47b8aaeef7d35f1cd5ce",
            "998e710a0ce3455a8b1d701729d5d042",
            "aed790d53f4144c1b4d74ce3b3a124bc",
            "8f4ce33740f944f38bfb6490432941dd",
            "b0eb10a001e14187b9960e065e83be69",
            "533e9c6c63064a4abf3080a11b9a7d36",
            "921933bbb9cb4b18bd5b4bbcb949a696",
            "94ea0a21452b4cddbf044c29bf830c18",
            "003d1d02216b40a68f7518b5b4b2eb75",
            "f354c3741edc44a0bfbee8457d7afd99",
            "18c080d9c9b64552a3923ec7aa1130f2"
          ]
        },
        "id": "E77d6UWTNc1Y",
        "outputId": "e2441411-b8f7-4e44-a388-598be2c8cd75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8067fa0d19a841bf9e2d8010e5b41538",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "003d1d02216b40a68f7518b5b4b2eb75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01c6d59642e54b91997487ddd344a9de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0298afd49407463a8343af570b5559dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04b0c5c6be02481e80639aac8f90c2e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04c174ca4a2042febab7b2722f39094c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70f5d0e37dad4ac295058095bcf4b37e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0298afd49407463a8343af570b5559dc",
            "value": "Token is valid (permission: write)."
          }
        },
        "0b8c87b555ba47b8aaeef7d35f1cd5ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94ea0a21452b4cddbf044c29bf830c18",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_003d1d02216b40a68f7518b5b4b2eb75",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "0edc9ae8a9f24098a9d5e2f0aa85ea9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "131c2d2e8eda419a86c2476a401e12bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18c080d9c9b64552a3923ec7aa1130f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a05991216104fd1afb579397fd4600b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_422d48bb5c1848fd86a1791ec42f7b62",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_642f0463720441dca0b4f40ea574fde1",
            "value": " 4.40G/4.40G [00:51&lt;00:00, 78.7MB/s]"
          }
        },
        "234237048fc8440e87b89d884ca4c2ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "23f789e3e1d2465fbacb48ed234d806e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7680a634cb441d8b617baedec1fffc3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ef7a8fab7974e76b20b764e58e37294",
            "value": " 68.0/68.0 [00:00&lt;00:00, 2.92kB/s]"
          }
        },
        "2b4d77408cdd48369f26f412403313ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ef7a8fab7974e76b20b764e58e37294": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36d632f5562044fd88e871cc7002c3b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca2260dc74b34286937c130287bbebe7",
              "IPY_MODEL_a549c51862e94b8ea44bce6a1fd5b8c7",
              "IPY_MODEL_23f789e3e1d2465fbacb48ed234d806e"
            ],
            "layout": "IPY_MODEL_570c05454066427499b33b8305d289ee"
          }
        },
        "381d5df4f32242fbba2bdb114626e79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f4ce33740f944f38bfb6490432941dd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b0eb10a001e14187b9960e065e83be69",
            "value": "Token is valid (permission: write)."
          }
        },
        "3ddac7a351f5429191f2da6bdeffe1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "417a4eefe03044d99f8413612d8601c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "422d48bb5c1848fd86a1791ec42f7b62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c0064a521d44c738b9f8a02f0676fca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52692a2d741e4ecbb98328a304344914": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "533e9c6c63064a4abf3080a11b9a7d36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "570c05454066427499b33b8305d289ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5899b5b82b3c4a57878fff9e250d9cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c0064a521d44c738b9f8a02f0676fca",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e2073cc48ec470b811a19cf524dc1d3",
            "value": "Login successful"
          }
        },
        "5dced4329d4e45a08d9df29bddf727c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04b0c5c6be02481e80639aac8f90c2e5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f8a556eeb6504acc9185a386215d7fc7",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "5e2073cc48ec470b811a19cf524dc1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5eb76005d7004cd68505cb733bd03a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d12e97901549b2b666370c802d9280",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3ddac7a351f5429191f2da6bdeffe1b0",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "6264895659d74c08bf0a11fe4ef2451b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642f0463720441dca0b4f40ea574fde1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70f5d0e37dad4ac295058095bcf4b37e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad5e754de4947158b7a082deaf58c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_533e9c6c63064a4abf3080a11b9a7d36",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_921933bbb9cb4b18bd5b4bbcb949a696",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "7c4665174325491789014d052bc9df0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8067fa0d19a841bf9e2d8010e5b41538": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_381d5df4f32242fbba2bdb114626e79a",
              "IPY_MODEL_7ad5e754de4947158b7a082deaf58c77",
              "IPY_MODEL_0b8c87b555ba47b8aaeef7d35f1cd5ce",
              "IPY_MODEL_998e710a0ce3455a8b1d701729d5d042"
            ],
            "layout": "IPY_MODEL_aed790d53f4144c1b4d74ce3b3a124bc"
          }
        },
        "82d12e97901549b2b666370c802d9280": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f4ce33740f944f38bfb6490432941dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "921933bbb9cb4b18bd5b4bbcb949a696": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94ea0a21452b4cddbf044c29bf830c18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98bb412076f944758f556be28c502679": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdff279416654bc8a7af2c074aae3c2f",
              "IPY_MODEL_d249d82f04b14a609bc03034e832156c",
              "IPY_MODEL_1a05991216104fd1afb579397fd4600b"
            ],
            "layout": "IPY_MODEL_01c6d59642e54b91997487ddd344a9de"
          }
        },
        "998e710a0ce3455a8b1d701729d5d042": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f354c3741edc44a0bfbee8457d7afd99",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_18c080d9c9b64552a3923ec7aa1130f2",
            "value": "Login successful"
          }
        },
        "a1d2cad6d24248d2ab0838d3db1da2ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a24f91722da543ebafee5bfe44355ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3e34dea245e4536a94db1897d1f1e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a83fca16ac4444f1a09e2cb136af1b8e",
              "IPY_MODEL_faad8277fc0c4e54abde25a8fb6bce27",
              "IPY_MODEL_e674401403004746807af85626ec30d8"
            ],
            "layout": "IPY_MODEL_7c4665174325491789014d052bc9df0e"
          }
        },
        "a549c51862e94b8ea44bce6a1fd5b8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d2cad6d24248d2ab0838d3db1da2ab",
            "max": 68,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a981ddf7e86040bd949d046a91523163",
            "value": 68
          }
        },
        "a83fca16ac4444f1a09e2cb136af1b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e08177adea504f5e84dd11dd6dd85c3a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0edc9ae8a9f24098a9d5e2f0aa85ea9b",
            "value": "config.json: 100%"
          }
        },
        "a981ddf7e86040bd949d046a91523163": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aed790d53f4144c1b4d74ce3b3a124bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b0eb10a001e14187b9960e065e83be69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7680a634cb441d8b617baedec1fffc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be5254474c0d4d919cf655ef0c904b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca2260dc74b34286937c130287bbebe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6264895659d74c08bf0a11fe4ef2451b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d522e19358464f0e985741ea429e183e",
            "value": "generation_config.json: 100%"
          }
        },
        "d249d82f04b14a609bc03034e832156c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_417a4eefe03044d99f8413612d8601c0",
            "max": 4400265688,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a24f91722da543ebafee5bfe44355ab2",
            "value": 4400265688
          }
        },
        "d522e19358464f0e985741ea429e183e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e08177adea504f5e84dd11dd6dd85c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e674401403004746807af85626ec30d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b4d77408cdd48369f26f412403313ca",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fa0c611f002a4b928845b5cff16db15b",
            "value": " 707/707 [00:00&lt;00:00, 35.0kB/s]"
          }
        },
        "ea035aa6e1914d3a889238e4944748d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04c174ca4a2042febab7b2722f39094c",
              "IPY_MODEL_5dced4329d4e45a08d9df29bddf727c0",
              "IPY_MODEL_5eb76005d7004cd68505cb733bd03a3e",
              "IPY_MODEL_5899b5b82b3c4a57878fff9e250d9cc2"
            ],
            "layout": "IPY_MODEL_234237048fc8440e87b89d884ca4c2ba"
          }
        },
        "f06b0d93264c426b879d5eb6c2cdb802": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f354c3741edc44a0bfbee8457d7afd99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8a556eeb6504acc9185a386215d7fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa0c611f002a4b928845b5cff16db15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faad8277fc0c4e54abde25a8fb6bce27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f06b0d93264c426b879d5eb6c2cdb802",
            "max": 707,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be5254474c0d4d919cf655ef0c904b60",
            "value": 707
          }
        },
        "fdff279416654bc8a7af2c074aae3c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131c2d2e8eda419a86c2476a401e12bf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_52692a2d741e4ecbb98328a304344914",
            "value": "model.safetensors: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}